{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+AWzlUDlMgoCtNSPbX99P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Risad-Raihan/2_layer_nn/blob/main/forward_and_backprop_for_2_layer_nns.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "RPO4uoSbuhdn"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "  \"\"\"Sigmoid Activation Function. \"\"\"\n",
        "  return 1/(1 + np.exp(-z))"
      ],
      "metadata": {
        "id": "jKlLC7C5ukLF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (sigmoid(1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUaCYwZhu6ex",
        "outputId": "67b94055-a9b6-49d8-bfd0-c6b8e53f8465"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7310585786300049\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XqmXqrBNvvdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid_derivative(a):\n",
        "  \"\"\"Derivative of Sigmoid Activation Function. \"\"\"\n",
        "  return a * (1 - a)"
      ],
      "metadata": {
        "id": "Z89XBOizu-wt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (sigmoid_derivative(sigmoid(1)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQyhMMCivUio",
        "outputId": "b1282389-9dfe-4afa-af94-3ede6847fa12"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.19661193324148185\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "does it stay"
      ],
      "metadata": {
        "id": "DxApJbfOvxM8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_parameters(n_x, n_h, n_y):\n",
        "  \"\"\"Initialize weights and bases. \"\"\"\n",
        "  W1 = np.random.randn(n_h, n_x)\n",
        "  b1 = np.zeros((n_h, 1))\n",
        "  W2 = np.random.randn(n_y, n_h)\n",
        "  b2 = np.zeros((n_y, 1))\n",
        "  parameters = {\"W1\": W1,\n",
        "                \"b1\": b1,\n",
        "                \"W2\": W2,\n",
        "                \"b2\": b2}\n",
        "  return parameters"
      ],
      "metadata": {
        "id": "qsJeI13gvbZi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pgzO5QO4wz_u"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Forward Propagation"
      ],
      "metadata": {
        "id": "1GUScFBXxNEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_propagation(X, parameters):\n",
        "  \"\"\"Forward Propagation. \"\"\"\n",
        "  W1 = parameters[\"W1\"]\n",
        "  b1 = parameters[\"b1\"]\n",
        "  W2 = parameters[\"W2\"]\n",
        "  b2 = parameters[\"b2\"]\n",
        "\n",
        "  # Layer 1\n",
        "  Z1 = np.dot(W1, X) + b1\n",
        "  A1 = np.tanh(Z1)\n",
        "\n",
        "  # Layer 2\n",
        "  Z2 = np.dot(W2, A1) + b2\n",
        "  A2 = sigmoid(Z2)\n",
        "\n",
        "  cache = {\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2}\n",
        "\n",
        "  return A2, cache"
      ],
      "metadata": {
        "id": "eK0dq2kHxOnv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost(A2, Y):\n",
        "  \"\"\"Compute binary cross-entropy Cost. \"\"\"\n",
        "  m = Y.shape[1]\n",
        "  cost = -(1/m) * np.sum(Y * np.log(A2) + (1-Y) * np.log(1-A2))\n",
        "  cost = np.squeeze(cost) #scaler\n",
        "  return cost\n"
      ],
      "metadata": {
        "id": "WKdd5rUpxk-0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "cost = -(1/m) * np.sum(Y * np.log(A2) + (1 - Y) * np.log(1 - A2)): Calculates the binary cross-entropy cost function. This formula measures the difference between the predicted probabilities (A2) and the true labels (Y). The sum is taken over all training examples, and then the result is averaged by dividing by m. The negative sign ensures that minimizing the cost leads to better predictions. np.log is the natural logarithm, and the multiplication and subtraction are performed element-wise."
      ],
      "metadata": {
        "id": "KNRGX4rmy0pk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wUAh9eHTydHS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Back Prop"
      ],
      "metadata": {
        "id": "E9RePq_Zy-7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_propagation(parameters, cache, X, Y):\n",
        "  m = X.shape[1]\n",
        "  W1 = parameters[\"W1\"]\n",
        "  W2 = parameters[\"W2\"]\n",
        "  A1 = cache[\"A1\"]\n",
        "  A2 = cache[\"A2\"]\n",
        "\n",
        "  #output layer - layer 2\n",
        "  dZ2 = A2 - Y\n",
        "  dW2 = (1/m) * np.dot(dZ2, A1.T)\n",
        "  db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
        "\n",
        "  #hidden layer - layer 1\n",
        "  dA1 = np.dot(W2.T, dZ2)\n",
        "  dZ1 = dA1 * sigmoid_derivative(A1)\n",
        "  dW1 = (1/m) * np.dot(dZ1, X.T)\n",
        "  db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
        "\n",
        "  grads = {\"dW1\": dW1,\n",
        "           \"db1\": db1,\n",
        "           \"dW2\": dW2,\n",
        "           \"db2\": db2}\n",
        "  return grads"
      ],
      "metadata": {
        "id": "OHiRCb5Vy_vQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KUIeI2_pD81c"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "dZ2 = A2 - Y: Calculates the error term for the output layer. For a sigmoid output with a binary cross-entropy loss, the derivative of the loss with respect to the linear output Z2 is simply A2âˆ’Y. The shape of dZ2 is (n_y, m).\n",
        "dW2 = (1/m) * np.dot(dZ2, A1.T): Calculates the gradient of the cost with respect to the weights W2. np.dot(dZ2, A1.T) performs the matrix multiplication of dZ2 (shape (n_y, m)) and the transpose of A1 (shape (m, n_h)), resulting in a matrix of shape (n_y, n_h), which is the same shape as W2. The (1/m) factor averages the gradients over all training examples.\n",
        "db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True): Calculates the gradient of the cost with respect to the bias b2. np.sum(dZ2, axis=1, keepdims=True) sums the error dZ2 along the rows (axis=1), resulting in a vector of shape (n_y, 1), which is the same shape as b2. keepdims=True ensures that the result has the correct dimensions for broadcasting during parameter update. The (1/m) factor averages the gradients.\n",
        "# Hidden Layer (Layer 1): Comment indicating calculations for the hidden layer.\n",
        "dA1 = np.dot(W2.T, dZ2): Calculates the derivative of the cost with respect to the activations of the hidden layer A1. np.dot(W2.T, dZ2) multiplies the transpose of W2 (shape (n_h, n_y)) with dZ2 (shape (n_y, m)), resulting in a matrix of shape (n_h, m), the same shape as A1. This propagates the error back to the previous layer.\n",
        "dZ1 = dA1 * sigmoid_derivative(A1): Calculates the derivative of the cost with respect to the linear output Z1 of the hidden layer. This is obtained by multiplying the upstream gradient dA1 element-wise with the derivative of the sigmoid activation function evaluated at A1. The * operator performs element-wise multiplication. The shape of dZ1 is (n_h, m).\n",
        "dW1 = (1/m) * np.dot(dZ1, X.T): Calculates the gradient of the cost with respect to the weights W1. np.dot(dZ1, X.T) multiplies dZ1 (shape (n_h, m)) with the transpose of the input X (shape (m, n_x)), resulting in a matrix of shape (n_h, n_x), the same shape as W1. The (1/m) factor averages the gradients.\n",
        "db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True): Calculates the gradient of the cost with respect to the bias b1. np.sum(dZ1, axis=1, keepdims=True) sums dZ1 along the rows (axis=1), resulting in a vector of shape (n_h, 1), the same shape as b1. The (1/m) factor averages the gradients.\n",
        "grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}: Creates a dictionary grads to store the calculated gradients for W1, b1, W2, and b2."
      ],
      "metadata": {
        "id": "C4LYD53GEq9a"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A1tQw5zUErXY"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_parameters(parameters, grads, learning_rate):\n",
        "  \"\"\"Update parameters using gradient descent. \"\"\"\n",
        "  W1 = parameters[\"W1\"]\n",
        "  b1 = parameters[\"b1\"]\n",
        "  W2 = parameters[\"W2\"]\n",
        "  b2 = parameters[\"b2\"]\n",
        "  dW1 = grads[\"dW1\"]\n",
        "  db1 = grads[\"db1\"]\n",
        "  dW2 = grads[\"dW2\"]\n",
        "  db2 = grads[\"db2\"]\n",
        "\n",
        "  W1 = W1 - learning_rate * dW1\n",
        "  b1 = b1 - learning_rate * db1\n",
        "  W2 = W2 - learning_rate * dW2\n",
        "  b2 = b2 - learning_rate * db2\n",
        "\n",
        "  parameters = {\"W1\": W1,\n",
        "                \"b1\": b1,\n",
        "                \"W2\": W2,\n",
        "                \"b2\": b2}\n",
        "  return parameters\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "sjhTIaKcE1S3"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uA2uQLmeFQjC"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Example Usage**"
      ],
      "metadata": {
        "id": "Ma5PLKxOFWaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  #structre of the netwprk\n",
        "  n_x = 2   # Number of input features\n",
        "  n_h = 4   # Number of neurons in the hidden layer\n",
        "  n_y = 1   # Number of neurons in the output layer (binary classification)\n",
        "  m = 100   # Number of training examples\n",
        "\n",
        "  #Generate some dummy data\n",
        "  X= np.random.randn(n_x, m)\n",
        "  Y = np.random.randint(0,2, size =(1,m))\n",
        "\n",
        "  #initialise parameters\n",
        "  parameters = initialize_parameters(n_x, n_h, n_y)\n",
        "\n",
        "  #set learning rate\n",
        "  learning_rate = 0.01\n",
        "\n",
        "  #training loop\n",
        "  num_iterations = 10000\n",
        "  for i in range(num_iterations):\n",
        "    #forward propagation\n",
        "    A2, cache = forward_propagation(X, parameters)\n",
        "\n",
        "    #compute cost\n",
        "    cost = compute_cost(A2, Y)\n",
        "\n",
        "    #Back prop\n",
        "    grads = backward_propagation(parameters, cache, X, Y)\n",
        "\n",
        "    #update parameters\n",
        "    parameters = update_parameters(parameters, grads, learning_rate)\n",
        "\n",
        "    #print cost every 100 num_iterations\n",
        "    if i % 100 == 0:\n",
        "      print(f\"Cost after iteration {i}: {cost}\")\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ruzgb1BNFX5g",
        "outputId": "212f4c1f-889b-463f-a92e-71e1d4c51ff1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost after iteration 0: 0.7777571291611867\n",
            "Cost after iteration 100: 0.7592389228829867\n",
            "Cost after iteration 200: 0.7474377696627231\n",
            "Cost after iteration 300: 0.7401474771370038\n",
            "Cost after iteration 400: 0.7358723670180937\n",
            "Cost after iteration 500: 0.733646432183807\n",
            "Cost after iteration 600: 0.732805254633658\n",
            "Cost after iteration 700: 0.7328038970428399\n",
            "Cost after iteration 800: 0.7331165773948444\n",
            "Cost after iteration 900: 0.7332339815454566\n",
            "Cost after iteration 1000: 0.7327459809517975\n",
            "Cost after iteration 1100: 0.731448552408488\n",
            "Cost after iteration 1200: 0.7293909270463661\n",
            "Cost after iteration 1300: 0.7268229608069322\n",
            "Cost after iteration 1400: 0.7240760706733451\n",
            "Cost after iteration 1500: 0.7214448217433211\n",
            "Cost after iteration 1600: 0.7191162523498826\n",
            "Cost after iteration 1700: 0.7171553271652921\n",
            "Cost after iteration 1800: 0.7155295996000233\n",
            "Cost after iteration 1900: 0.7141520650814682\n",
            "Cost after iteration 2000: 0.712927551088335\n",
            "Cost after iteration 2100: 0.7117901466074722\n",
            "Cost after iteration 2200: 0.7107171107002747\n",
            "Cost after iteration 2300: 0.7097151684344679\n",
            "Cost after iteration 2400: 0.708796341515887\n",
            "Cost after iteration 2500: 0.7079636118137234\n",
            "Cost after iteration 2600: 0.7072098795589298\n",
            "Cost after iteration 2700: 0.7065227046855413\n",
            "Cost after iteration 2800: 0.7058887981390035\n",
            "Cost after iteration 2900: 0.7052965246774342\n",
            "Cost after iteration 3000: 0.7047367781933633\n",
            "Cost after iteration 3100: 0.7042029544005829\n",
            "Cost after iteration 3200: 0.7036905467552603\n",
            "Cost after iteration 3300: 0.7031966533473519\n",
            "Cost after iteration 3400: 0.7027195243602496\n",
            "Cost after iteration 3500: 0.7022581941126428\n",
            "Cost after iteration 3600: 0.7018122014858497\n",
            "Cost after iteration 3700: 0.7013813867351835\n",
            "Cost after iteration 3800: 0.7009657486732018\n",
            "Cost after iteration 3900: 0.7005653472254812\n",
            "Cost after iteration 4000: 0.7001802390542079\n",
            "Cost after iteration 4100: 0.6998104368214146\n",
            "Cost after iteration 4200: 0.6994558851463266\n",
            "Cost after iteration 4300: 0.6991164482549138\n",
            "Cost after iteration 4400: 0.6987919057589943\n",
            "Cost after iteration 4500: 0.698481954031326\n",
            "Cost after iteration 4600: 0.6981862113630878\n",
            "Cost after iteration 4700: 0.6979042255884477\n",
            "Cost after iteration 4800: 0.6976354832062718\n",
            "Cost after iteration 4900: 0.6973794192721693\n",
            "Cost after iteration 5000: 0.6971354275106841\n",
            "Cost after iteration 5100: 0.6969028702314731\n",
            "Cost after iteration 5200: 0.6966810877399626\n",
            "Cost after iteration 5300: 0.6964694070212224\n",
            "Cost after iteration 5400: 0.6962671495504966\n",
            "Cost after iteration 5500: 0.6960736381472957\n",
            "Cost after iteration 5600: 0.6958882028430297\n",
            "Cost after iteration 5700: 0.6957101857753029\n",
            "Cost after iteration 5800: 0.6955389451553681\n",
            "Cost after iteration 5900: 0.6953738583794296\n",
            "Cost after iteration 6000: 0.6952143243701734\n",
            "Cost after iteration 6100: 0.6950597652431688\n",
            "Cost after iteration 6200: 0.6949096273947373\n",
            "Cost after iteration 6300: 0.6947633821048494\n",
            "Cost after iteration 6400: 0.6946205257417473\n",
            "Cost after iteration 6500: 0.6944805796455059\n",
            "Cost after iteration 6600: 0.6943430897566221\n",
            "Cost after iteration 6700: 0.6942076260438317\n",
            "Cost after iteration 6800: 0.6940737817734809\n",
            "Cost after iteration 6900: 0.6939411726514401\n",
            "Cost after iteration 7000: 0.6938094358582931\n",
            "Cost after iteration 7100: 0.693678228989683\n",
            "Cost after iteration 7200: 0.6935472289065052\n",
            "Cost after iteration 7300: 0.6934161304942827\n",
            "Cost after iteration 7400: 0.6932846453275464\n",
            "Cost after iteration 7500: 0.6931525002333412\n",
            "Cost after iteration 7600: 0.6930194357479147\n",
            "Cost after iteration 7700: 0.6928852044619704\n",
            "Cost after iteration 7800: 0.6927495692522849\n",
            "Cost after iteration 7900: 0.6926123014006729\n",
            "Cost after iteration 8000: 0.6924731786048995\n",
            "Cost after iteration 8100: 0.6923319828900061\n",
            "Cost after iteration 8200: 0.6921884984324218\n",
            "Cost after iteration 8300: 0.6920425093133313\n",
            "Cost after iteration 8400: 0.6918937972222214\n",
            "Cost after iteration 8500: 0.6917421391369102\n",
            "Cost after iteration 8600: 0.6915873050134514\n",
            "Cost after iteration 8700: 0.6914290555292182\n",
            "Cost after iteration 8800: 0.691267139936774\n",
            "Cost after iteration 8900: 0.6911012941067571\n",
            "Cost after iteration 9000: 0.6909312388674971\n",
            "Cost after iteration 9100: 0.6907566787905263\n",
            "Cost after iteration 9200: 0.6905773016284198\n",
            "Cost after iteration 9300: 0.6903927786888688\n",
            "Cost after iteration 9400: 0.6902027665315938\n",
            "Cost after iteration 9500: 0.6900069105073239\n",
            "Cost after iteration 9600: 0.6898048508241493\n",
            "Cost after iteration 9700: 0.6895962320257637\n",
            "Cost after iteration 9800: 0.6893807169904623\n",
            "Cost after iteration 9900: 0.6891580067869798\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    #Make prediction\n",
        "    A2_final, _ = forward_propagation(X, parameters)\n",
        "    predictions = (A2_final > 0.5) * 1\n",
        "\n",
        "    #calculate accuracy\n",
        "    accuracy = np.mean((predictions == Y).astype(int))\n",
        "\n",
        "    print (f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpWBvWmKGzcR",
        "outputId": "1c7ef192-1d96-466d-bfdb-ff567cc87d9e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.52\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "poAxtJVjHsyg"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}